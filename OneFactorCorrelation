#install necessary packages
install.packages("googleAnalyticsR", dependencies=TRUE)
install.packages("stringr")
install.packages("dplyr")
#install.packages("plyr")
install.packages("ggplot2")
install.packages("ggpubr")
install.packages("data.table")

#load necessary libraries
library(googleAnalyticsR)
library(stringr)
library(dplyr)
#library(plyr)
library(ggplot2)
library(ggpubr)
library(readxl)
library(data.table)


#authorizes R to establish a connection with our GA account
ga_auth()

#brand view ID
ga_id <- 186738676

#test query
google_analytics(ga_id, 
                 date_range = c("2019-12-01", "2020-01-01"), 
                 metrics = "sessions", 
                 dimensions = c("year","month","hostname"))

test <- ga_filter_view_list(1341196, "UA-1341196-19", 186738676)

#create a dataframe that contains all the segments IDs that we will be pulling
all.segments <- as.data.frame(ga_segment_list())

hev.segments.allfields <- subset(all.segments,grepl("HEV",name))

#add index to all hev segments to better keep track of them
hev.segments <- select(hev.segments.allfields,id,segmentId,name)
hev.segments.index <- data.frame(c(1:length(hev.segments$name)))
names(hev.segments.index)[1] <-"index"
hev.segments <- cbind(hev.segments,hev.segments.index)
names(hev.segments)[3] <- "segmentname"  

#test query with segment and unsampled data
test <- segment_ga4("FCA 2020 HEV 4x4  Story module clicks", segment_id = "gaid::nfqCa3bSQGWRCsIAksdv3w")

test_segment <- google_analytics(ga_id, 
                                   date_range = c("2019-12-01", "2020-01-01"), 
                                   metrics = "sessions", 
                                   dimensions = c("year","month","hostname"),
                                   segments = test,
                                   filtersExpression = "ga:hostname==www.jeep.ca",
                                   anti_sample = TRUE)

#start querying the GA reporting API against all hev segments by brand, so that we can look at one to one correlation first

list.sessions <- list()
list.year <- list()
list.month <- list()
list.segmentname <- list()
list.hevnames <- list()

#jeep query
i=1
#after testing, change 1:3 to 1:nrow(hev.segments)
for (i in 1:3){
  segmentdata <- google_analytics(ga_id, 
                           date_range = c("2019-01-01", "2020-02-29"), 
                           metrics = "sessions", 
                           dimensions = c("year","month"),
                           segments = segment_ga4(hev.segments$segmentname[i], segment_id = hev.segments$segmentId[i]),
                           filtersExpression = "ga:hostname==www.jeep.ca",
                           anti_sample = TRUE)
  
    #segmentnum <- paste0("jeep","hevsegment", i)
    segmentdata <- as.data.frame(segmentdata)
    
    segmentsessions <- segmentdata$sessions
    segmentyear <- segmentdata$year
    segmentmonth <- segmentdata$month
    segmentname <- segmentdata$segment

    list.sessions <- append(list.sessions,segmentsessions)
    list.year <- append(list.year,segmentyear)
    list.month <- append(list.month,segmentmonth)
    list.segmentname <- append(list.segmentname,segmentname)

  i = i+1
}

list.sessions <- do.call(rbind, list.sessions)
colnames(list.sessions) <- c("sessions")

list.year <- do.call(rbind, list.year)
colnames(list.year) <- c("year")

list.month <- do.call(rbind, list.month)
colnames(list.month) <- c("month")

list.segmentname <- do.call(rbind, list.segmentname)
colnames(list.segmentname) <- c("segmentname")

#bind all of our columns together
jeephevdata <- data.frame(list.segmentname,list.year,list.month,list.sessions)
all.jeephevdata <- jeephevdata

all.jeephevdata <- merge(all.jeephevdata,hev.segments)

#for each hev segment, calculate some basic statistics (min, max, average, variance, 
#number of data points, number of extreme outliers, shapiro wilk statistic)
#pearson correlation coefficient has the following assumptions: 
#normality of variables, linearity, and homoscedasticity
#to test the normality of our hev segments, we will use the shapiro wilk test

jeepmean <- aggregate( sessions ~ segmentname, jeephevdata, mean )
jeepmax <- aggregate( sessions ~ segmentname, jeephevdata, max )
jeepmin <- aggregate( sessions ~ segmentname, jeephevdata, min )
jeepvariance <- aggregate( sessions ~ segmentname, jeephevdata, var )
jeepcount <- aggregate( sessions ~ segmentname, jeephevdata, length )


#make a function that defines the lower bound for an outlier
jeepoutlierlowerbound <- function(x){
  quantile(x, 0.25)*1.5
}
jeepoutlierlowerbound <- aggregate( sessions ~ segmentname, jeephevdata, jeepoutlierlowerbound )
names(jeepoutlierlowerbound)[2] <- "lowerbound"


#make a function that defines the upper bound for an outlier
jeepoutlierupperbound <- function(x){
  quantile(x, 0.75)*1.5
}
jeepoutlierupperbound <- aggregate( sessions ~ segmentname, jeephevdata, jeepoutlierupperbound )
names(jeepoutlierupperbound)[2] <- "upperbound"


#now left join these upper and lower bounds to our main dataset so that we can flag which values fall above and below the bounds
jeephevdata <-merge(x=jeephevdata,y=jeepoutlierupperbound,by="segmentname",all.x=TRUE)
jeephevdata <-merge(x=jeephevdata,y=jeepoutlierlowerbound,by="segmentname",all.x=TRUE)


#make a function to flag data points that are lower than the lower bound
#make sessions the x value and the lower bound the y value

#lower bound flag
lowerbound.function <- function(x,y){
  ifelse(x<y,1,0)
}

lowerflags <- data.frame(mapply(lowerbound.function,jeephevdata$sessions,jeephevdata$lowerbound))
names(lowerflags)[1] <- "lowerboundflag"

#bind our lowerbound flags to the original dataframe
jeephevdata <- cbind(jeephevdata,lowerflags)


#do the same thing for the upper bound now
#make sessions the x value and the upper bound the y value

#upper bound flag
upperbound.function <- function(x,y){
  ifelse(x>y,1,0)
}

upperflags <- data.frame(mapply(upperbound.function,jeephevdata$sessions,jeephevdata$upperbound))
names(upperflags)[1] <- "upperboundflag"

#bind our upperbound flags to the original dataframe
jeephevdata <- cbind(jeephevdata,upperflags)


#now that we have both upper and lower bound flags, count the number of flags ie. the number of upper and lower outliers we have
jeepupperoutliers <- aggregate( upperboundflag ~ segmentname, jeephevdata, sum )
jeeploweroutliers <- aggregate( lowerboundflag ~ segmentname, jeephevdata, sum )

#define function for shapiro wilk statistic
jeepshapiro <- function(x){
  shapiro <- shapiro.test(x)
  #this returns the p-value of the shapiro wilk test
  return(shapiro[[2]])
}
jeepshapiro <- aggregate( sessions ~ segmentname, jeephevdata, jeepshapiro )


#check to make sure that we've calculated our parameters for all of our segments
stopifnot(length(jeepmean)==length(jeepmax))
stopifnot(length(jeepmean)==length(jeepmin))
stopifnot(length(jeepmean)==length(jeepvariance))
stopifnot(length(jeepmean)==length(jeepcount))
stopifnot(length(jeepmean)==length(jeepoutlierlowerbound))
stopifnot(length(jeepmean)==length(jeepoutlierupperbound))
stopifnot(length(jeepmean)==length(jeepshapiro))
stopifnot(length(jeepmean)==length(jeepupperoutliers))
stopifnot(length(jeepmean)==length(jeeploweroutliers))

#before merging all our summary stat dataframes together, we will rename the columns accordingly
names(jeepmean)[2] <- "mean"
names(jeepmax)[2] <- "max"
names(jeepmin)[2] <- "min"
names(jeepvariance)[2] <- "variance"
names(jeepcount)[2] <- "count"
names(jeepshapiro)[2] <- "shapiro"
names(jeepupperoutliers)[2] <- "upperoutliers"
names(jeeploweroutliers)[2] <- "loweroutliers"



#now that we've calculated all our summary statistics, let's compile them into one dataframe
jeep.summarystats <- merge(jeepmean,jeepmax,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeepmin,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeepvariance,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeepcount,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeepshapiro,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeepupperoutliers,by="segmentname")
jeep.summarystats <- merge(jeep.summarystats,jeeploweroutliers,by="segmentname")

#export summary statistics for each of our segments as an excel so that the team can determine 
#which should be included or excluded for further analysis
write.csv(jeep.summarystats, file = "Desktop/jeep-segmentsummarystats.csv")


#since our sample size for each segment is small, we will also graph each segment's session values to assess normality
segment_list <-unique(jeephevdata$segmentname)

#create a for loop to produce density plots and save on desktop
for (i in seq_along(segment_list)) {
  plot <- ggplot(subset(jeephevdata,segmentname==segment_list[i]), aes(x=sessions)) + 
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) + ggtitle(jeephevdata$segmentname[i])
  ggsave(paste0("jeepdensity",i,".png"),path = "Desktop")
  
}

#-------------------------------------------------------------------------------------------------------------------------------------#
#ADD SOME CODE HERE TO DO THE SAME THING FOR QQ NORM PLOTS
#-------------------------------------------------------------------------------------------------------------------------------------#

#do the same thing for normal qq plots
#for (i in seq_along(segment_list)) {
#  plot <- ggplot(subset(jeephevdata,segmentname==segment_list[i]), aes(x=sessions)) + 
#    stat_qq() + ggtitle(jeephevdata$segmentname[i])
#  ggsave(paste0("jeepqqnorm",i,".png"),path = "Desktop")
#}

#-------------------------------------------------------------------------------------------------------------------------------------#
#-------------------------------------------------------------------------------------------------------------------------------------#

#outside of R, determine which segments should be kept and which should be thrown out
#in the csv file that we exported, delete the rows that we no longer want to analyze further
#rename our csv file with the deleted rows as "jeep-chosensegments" and import
chosensegments <- read_excel("Desktop/jeep-chosensegments.xlsx")
chosensegments <- data.frame(chosensegments$segmentname)
colnames(chosensegments) <- c("segmentname")

#now go back to our initial file where we were collecting all segment session data and drop the segments that we no longer require
jeephevdata <- merge(jeephevdata,chosensegments,by="segmentname")

#import sales data from desktop
hevsales <- read_excel("Desktop/hevsales.xlsx")

#subset sales data so it's only jeep data
jeep.hevsales <- subset(hevsales, brand=="JEEP")

#join sales data to our hev segments
jeep.hevsegments.sales <- merge(jeephevdata,jeep.hevsales,by=c("year","month"))

#calculate correlation coefficients for our segments
jeepcorr <- function(x,y){
  cor(x, y, method = "pearson")
}


detach(package:plyr)
jeepcorrelation <- jeep.hevsegments.sales %>% group_by(segmentname) %>% summarize(jeepcorr(sessions, retailsales))
#rename the correlation column  
colnames(jeepcorrelation) <- c("segmentname","correlation")

#now that correlation coefficients have been calculated, categorize by weak, medium, strong correlation
correlationthresholds <- c(-1,0.5,0.7,1)
corrlabels <- c("weak","moderate","strong")

setDT(jeepcorrelation)[ , corrcategory := cut(correlation, 
                                breaks = correlationthresholds, 
                                right = FALSE, 
                                labels = corrlabels)]

#export file of categorized segments
write.csv(jeepcorrelation, file = "Desktop/jeepcorrelations-categorized.csv")
