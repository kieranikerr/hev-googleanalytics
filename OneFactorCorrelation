#install necessary packages
install.packages("googleAnalyticsR", dependencies=TRUE)
install.packages("stringr")
install.packages("dplyr")
#install.packages("plyr")
install.packages("ggplot2")
install.packages("ggpubr")
install.packages("data.table")

#load necessary libraries
library(googleAnalyticsR)
library(stringr)
library(dplyr)
#library(plyr)
library(ggplot2)
library(ggpubr)
library(readxl)
library(data.table)


#authorizes R to establish a connection with our GA account
ga_auth()

#brand view ID
ga_id <- 186738676

#test query
google_analytics(ga_id, 
                 date_range = c("2019-12-01", "2020-01-01"), 
                 metrics = "sessions", 
                 dimensions = c("year","month","hostname"))

test <- ga_filter_view_list(1341196, "UA-1341196-19", 186738676)

#create a dataframe that contains all the segments IDs that we will be pulling
all.segments <- as.data.frame(ga_segment_list())

hev.segments.allfields <- subset(all.segments,grepl("HEV",name))

#add index to all hev segments to better keep track of them
hev.segments <- select(hev.segments.allfields,id,segmentId,name)
hev.segments.index <- data.frame(c(1:length(hev.segments$name)))
names(hev.segments.index)[1] <-"index"
hev.segments <- cbind(hev.segments,hev.segments.index)
names(hev.segments)[3] <- "segmentname"  

#test query with segment and unsampled data
test <- segment_ga4("FCA 2020 HEV 4x4  Story module clicks", segment_id = "gaid::nfqCa3bSQGWRCsIAksdv3w")

test_segment <- google_analytics(ga_id, 
                                   date_range = c("2019-12-01", "2020-01-01"), 
                                   metrics = "sessions", 
                                   dimensions = c("year","month","hostname"),
                                   segments = test,
                                   filtersExpression = "ga:hostname==www.dodge.ca",
                                   anti_sample = TRUE)

#start querying the GA reporting API against all hev segments by brand, so that we can look at one to one correlation first


dodgehevdata <- data.frame()


#dodge query
i=1
#after testing, change 1:3 to 1:nrow(hev.segments)
for (i in 1:nrow(hev.segments)){
  segmentdata <- google_analytics(ga_id, 
                           date_range = c("2019-01-01", "2020-02-29"), 
                           metrics = "sessions", 
                           dimensions = c("year","month"),
                           segments = segment_ga4(hev.segments$segmentname[i], segment_id = hev.segments$segmentId[i]),
                           filtersExpression = "ga:hostname==www.dodge.ca",
                           anti_sample = TRUE)
  
    #segmentnum <- paste0("dodge","hevsegment", i)
    segmentdata <- as.data.frame(segmentdata)
    
    dodgehevdata <- bind_rows(dodgehevdata, segmentdata)

  i = i+1
}

#rename segment to segment name
names(dodgehevdata)[1] <- c("segmentname")



#for each hev segment, calculate some basic statistics (min, max, average, variance, 
#number of data points, number of extreme outliers, shapiro wilk statistic)
#pearson correlation coefficient has the following assumptions: 
#normality of variables, linearity, and homoscedasticity
#to test the normality of our hev segments, we will use the shapiro wilk test

dodgemean <- aggregate( sessions ~ segmentname, dodgehevdata, mean )
dodgemax <- aggregate( sessions ~ segmentname, dodgehevdata, max )
dodgemin <- aggregate( sessions ~ segmentname, dodgehevdata, min )
dodgevariance <- aggregate( sessions ~ segmentname, dodgehevdata, var )
dodgecount <- aggregate( sessions ~ segmentname, dodgehevdata, length )


#make a function that defines the lower bound for an outlier
dodgeoutlierlowerbound <- function(x){
  quantile(x, 0.25)*1.5
}
dodgeoutlierlowerbound <- aggregate( sessions ~ segmentname, dodgehevdata, dodgeoutlierlowerbound )
names(dodgeoutlierlowerbound)[2] <- "lowerbound"


#make a function that defines the upper bound for an outlier
dodgeoutlierupperbound <- function(x){
  quantile(x, 0.75)*1.5
}
dodgeoutlierupperbound <- aggregate( sessions ~ segmentname, dodgehevdata, dodgeoutlierupperbound )
names(dodgeoutlierupperbound)[2] <- "upperbound"


#now left join these upper and lower bounds to our main dataset so that we can flag which values fall above and below the bounds
dodgehevdata <-merge(x=dodgehevdata,y=dodgeoutlierupperbound,by="segmentname",all.x=TRUE)
dodgehevdata <-merge(x=dodgehevdata,y=dodgeoutlierlowerbound,by="segmentname",all.x=TRUE)


#make a function to flag data points that are lower than the lower bound
#make sessions the x value and the lower bound the y value

#lower bound flag
lowerbound.function <- function(x,y){
  ifelse(x<y,1,0)
}

lowerflags <- data.frame(mapply(lowerbound.function,dodgehevdata$sessions,dodgehevdata$lowerbound))
names(lowerflags)[1] <- "lowerboundflag"

#bind our lowerbound flags to the original dataframe
dodgehevdata <- cbind(dodgehevdata,lowerflags)


#do the same thing for the upper bound now
#make sessions the x value and the upper bound the y value

#upper bound flag
upperbound.function <- function(x,y){
  ifelse(x>y,1,0)
}

upperflags <- data.frame(mapply(upperbound.function,dodgehevdata$sessions,dodgehevdata$upperbound))
names(upperflags)[1] <- "upperboundflag"

#bind our upperbound flags to the original dataframe
dodgehevdata <- cbind(dodgehevdata,upperflags)


#now that we have both upper and lower bound flags, count the number of flags ie. the number of upper and lower outliers we have
dodgeupperoutliers <- aggregate( upperboundflag ~ segmentname, dodgehevdata, sum )
dodgeloweroutliers <- aggregate( lowerboundflag ~ segmentname, dodgehevdata, sum )

#define function for shapiro wilk statistic
dodgeshapiro <- function(x){
  shapiro <- shapiro.test(x)
  #this returns the p-value of the shapiro wilk test
  return(shapiro[[2]])
}
dodgeshapiro <- aggregate( sessions ~ segmentname, dodgehevdata, dodgeshapiro )


#check to make sure that we've calculated our parameters for all of our segments
stopifnot(length(dodgemean)==length(dodgemax))
stopifnot(length(dodgemean)==length(dodgemin))
stopifnot(length(dodgemean)==length(dodgevariance))
stopifnot(length(dodgemean)==length(dodgecount))
stopifnot(length(dodgemean)==length(dodgeoutlierlowerbound))
stopifnot(length(dodgemean)==length(dodgeoutlierupperbound))
stopifnot(length(dodgemean)==length(dodgeshapiro))
stopifnot(length(dodgemean)==length(dodgeupperoutliers))
stopifnot(length(dodgemean)==length(dodgeloweroutliers))

#before merging all our summary stat dataframes together, we will rename the columns accordingly
names(dodgemean)[2] <- "mean"
names(dodgemax)[2] <- "max"
names(dodgemin)[2] <- "min"
names(dodgevariance)[2] <- "variance"
names(dodgecount)[2] <- "count"
names(dodgeshapiro)[2] <- "shapiro"
names(dodgeupperoutliers)[2] <- "upperoutliers"
names(dodgeloweroutliers)[2] <- "loweroutliers"



#now that we've calculated all our summary statistics, let's compile them into one dataframe
dodge.summarystats <- merge(dodgemean,dodgemax,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgemin,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgevariance,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgecount,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgeshapiro,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgeupperoutliers,by="segmentname")
dodge.summarystats <- merge(dodge.summarystats,dodgeloweroutliers,by="segmentname")

#export summary statistics for each of our segments as an excel so that the team can determine 
#which should be included or excluded for further analysis
write.csv(dodge.summarystats, file = "Desktop/dodge-segmentsummarystats.csv")


#since our sample size for each segment is small, we will also graph each segment's session values to assess normality
segment_list <-unique(dodgehevdata$segmentname)

#create a for loop to produce density plots and save on desktop
for (i in seq_along(segment_list)) {
  plot <- ggplot(subset(dodgehevdata,segmentname==segment_list[i]), aes(x=sessions)) + 
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) + ggtitle(dodgehevdata$segmentname[i])
  ggsave(paste0("dodgedensity",i,".png"),path = "Desktop")
  
}

#-------------------------------------------------------------------------------------------------------------------------------------#
#ADD SOME CODE HERE TO DO THE SAME THING FOR QQ NORM PLOTS
#-------------------------------------------------------------------------------------------------------------------------------------#

#do the same thing for normal qq plots
#for (i in seq_along(segment_list)) {
#  plot <- ggplot(subset(dodgehevdata,segmentname==segment_list[i]), aes(x=sessions)) + 
#    stat_qq() + ggtitle(dodgehevdata$segmentname[i])
#  ggsave(paste0("dodgeqqnorm",i,".png"),path = "Desktop")
#}

#-------------------------------------------------------------------------------------------------------------------------------------#
#-------------------------------------------------------------------------------------------------------------------------------------#

#outside of R, determine which segments should be kept and which should be thrown out
#in the csv file that we exported, delete the rows that we no longer want to analyze further
#rename our csv file with the deleted rows as "dodge-chosensegments" and import
chosensegments <- read_excel("Desktop/dodge-chosensegments.xlsx")
chosensegments <- data.frame(chosensegments$segmentname)
colnames(chosensegments) <- c("segmentname")

#now go back to our initial file where we were collecting all segment session data and drop the segments that we no longer require
dodgehevdata <- merge(dodgehevdata,chosensegments,by="segmentname")

#import sales data from desktop
hevsales <- read_excel("Desktop/hevsales.xlsx")

#subset sales data so it's only dodge data
dodge.hevsales <- subset(hevsales, brand=="DODGE")

#join sales data to our hev segments
dodge.hevsegments.sales <- merge(dodgehevdata,dodge.hevsales,by=c("year","month"))

#calculate correlation coefficients for our segments
dodgecorr <- function(x,y){
  cor(x, y, method = "pearson")
}


detach(package:plyr)
dodgecorrelation <- dodge.hevsegments.sales %>% group_by(segmentname) %>% summarize(dodgecorr(sessions, retailsales))
#rename the correlation column  
colnames(dodgecorrelation) <- c("segmentname","correlation")

#now that correlation coefficients have been calculated, categorize by weak, medium, strong correlation
correlationthresholds <- c(-1,0.5,0.7,1)
corrlabels <- c("weak","moderate","strong")

setDT(dodgecorrelation)[ , corrcategory := cut(correlation, 
                                breaks = correlationthresholds, 
                                right = FALSE, 
                                labels = corrlabels)]

#export file of categorized segments
write.csv(dodgecorrelation, file = "Desktop/dodgecorrelations-categorized.csv")
